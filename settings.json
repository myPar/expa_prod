{
    "server_settings": {
        "vllm_server_url": "http://localhost:8001",
        "openai_api_key": "token-abc123",
        "proper_chat_format": true,
        "client_timeout": 600
    },
    "default_model_settings": {
        "temperature": 0.6,
        "top_p": 0.95,
        "max_completion_tokens": 1024,
        "stop": []
    }
}
