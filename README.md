# OpenAI-Compatible Proxy Server for vLLM

Этот репозиторий содержит прокси-сервер для работы с DeepSeek-r1 и его дистиллятами, совместимый с OpenAI API, который перенаправляет запросы к локальному серверу [vLLM](https://github.com/vllm-project/vllm). Можно развернуть как прокси сервер вместе с vllm сервером, так и просто vllm сервер отдельно и отправлять запросы напрямую ему.

Прокси сервер реализует необходимую обработку запросов для соответствия [рекомендациям](https://docs.together.ai/docs/prompting-deepseek-r1) по использованию r1.
Данный функционал можно настраивать через аргументы запросов к прокси серверу:

### POST `/v1/chat/completions`

### Описание
Этот эндпойнт обрабатывает запрос на генерацию ответа от языковой модели в формате чатового диалога. Имеет поддержку дополнительных параметров под серию моделей **R1**.

Метод поддерживает базовые параметры генерации, такие как `temperature`, `top_p`, `stop`-токены, а также ограничение количества токенов в ответе. Поддержка `tools` отключена — при их передаче будет возвращена ошибка.

---

### Метод: `POST`  
### Путь: `/v1/chat/completions`  
### Тип тела запроса: `application/json`  
### Ответ: `application/json`

### **Поля запроса:**

| Поле                  | Тип                           | Обязательное | Описание                                                                 |
|-----------------------|-------------------------------|--------------|--------------------------------------------------------------------------|
| `messages`            | `list[dict]`           | ✅ Да        | Список сообщений диалога (минимум 1). Первое сообщение — системное.      |
| `temperature`         | `float`                       | ❌ Нет       | Значение от 0.0 до 2.0. Управляет креативностью генерации.               |
| `max_completion_tokens` | `int`                       | ❌ Нет       | Максимальное число токенов в ответе.                                     |
| `top_p`               | `float`                       | ❌ Нет       | Порог nucleus sampling, от 0.0 до 1.0.                                   |
| `stop`                | `str` или `list[str]`         | ❌ Нет       | Строка или список строк для остановки генерации.                         |
| `n`                   | `int`                         | ❌ Нет       | Количество альтернативных ответов. По умолчанию — 1.                     |
| `stream`              | `bool`                        | ❌ Нет       | Стримить ли ответ (по умолчанию `False`).                                |
| `presence_penalty`    | `float`                       | ❌ Нет       | От -2.0 до 2.0. Повышает разнообразие тем.                                |
| `frequency_penalty`   | `float`                       | ❌ Нет       | От -2.0 до 2.0. Снижает частоту повторений.                               |
| `logit_bias`          | `dict`                        | ❌ Нет       | Смещение вероятностей для определённых токенов.                          |
| `user`                | `str`                         | ❌ Нет       | Идентификатор пользователя.                                              |
| `tools`               | `list[dict]`                  | ❌ Нет       | ⚠️ Не поддерживается для R1. При наличии будет ошибка.                   |
| `r1_settings`         | `dict`              | ❌ Нет       | Специальные настройки для моделей R1. См. следующую таблицу.             |

### `r1_settings`:

| Поле               | Тип                                                 | Описание                                                                 |
|--------------------|-----------------------------------------------------|--------------------------------------------------------------------------|
| `math_mode`        | `bool`                                              | Если `True`, активируется режим обработки математических выражений. В поле _content_ будет только ответ на задачу.    |
| `return_think_data`| `bool`                                              | Если `True`, возвращаются промежуточные размышления модели в поле _reasoning_content_.              |
| `few_shot_mode`    | `"DROP"` \| `"PREPROCESS"` \| `"NO_PREPROCESS"`    | Режим few-shot: убирать промежуточные элементы диалога - оставляет только последний user-prompt сконкатенированный с system-prompt; предварительно обрабатывать - конвертирование диалога в один user-prompt; никак не обрабатывать. По умолчанию: "NO_PREPROCESS" |

### POST `/v1/completions`

### Описание
Этот эндпойнт предоставляет доступ к генерации текста по аналогии с OpenAI Text Completion API (⚠️ является устаревшим).

Поддерживает настройки генерации (`temperature`, `top_p`, `stop`, `n` и др.) и также содержит специальные поля под серию моделей **R1**.

---

### Метод: `POST`  
### Путь: `/v1/completions`  
### Тип тела запроса: `application/json`  
### Ответ: `application/json`

---

### Поля запроса:

| Поле                 | Тип                              | Обязательное | Описание                                                                 |
|----------------------|----------------------------------|--------------|--------------------------------------------------------------------------|
| `prompt`             | `str` или `list[str]`            | ✅ Да        | Входная строка или список строк запросов.                    |
| `temperature`        | `float`                          | ❌ Нет       | Значение от 0.0 до 2.0. Управляет креативностью генерации.               |
| `max_completion_tokens` | `int`                         | ❌ Нет       | Максимальное число токенов в ответе.                                     |
| `top_p`              | `float`                          | ❌ Нет       | Порог nucleus sampling, от 0.0 до 1.0.                                   |
| `stop`               | `str` или `list[str]`            | ❌ Нет       | Строка или список строк для остановки генерации.                         |
| `n`                  | `int`                            | ❌ Нет       | Количество альтернативных ответов. По умолчанию — 1.                     |
| `stream`             | `bool`                           | ❌ Нет       | Стримить ли ответ (по умолчанию `False`).                                |
| `logprobs`           | `int`                            | ❌ Нет       | Если указано, возвращаются вероятности `logprobs` для топ-N токенов.     |
| `echo`               | `bool`                           | ❌ Нет       | Если `True`, возвращается и входной `prompt`.                            |
| `presence_penalty`   | `float`                          | ❌ Нет       | От -2.0 до 2.0. Повышает разнообразие тем.                                |
| `frequency_penalty`  | `float`                          | ❌ Нет       | От -2.0 до 2.0. Снижает частоту повторений.                               |
| `best_of`            | `int`                            | ❌ Нет       | Генерирует `best_of` ответов и выбирает лучший (работает при `n=1`).     |
| `logit_bias`         | `dict`                           | ❌ Нет       | Смещение вероятностей для определённых токенов.                          |
| `user`               | `str`                            | ❌ Нет       | Идентификатор пользователя.                                              |
| `r1_settings`        | `dict`                     | ❌ Нет       | Специальные настройки для моделей R1. См. ниже.                          |

---

### `r1_settings`

| Поле               | Тип     | Описание                                                           |
|--------------------|----------|---------------------------------------------------------------------|
| `math_mode`        | `bool`   | Если `True`, активируется режим обработки математических выражений. |
| `return_think_data`| `bool`   | Если `True`, возвращаются промежуточные размышления модели.         |

---


### Установка зависимостей

клонируем репозиторий:
```bash
git clone https://github.com/myPar/OpenAI_proxy.git
cd open_ai_proxy_r1
```

Для работы лучше создать отдельное окружение с **python 3.12**. Удобнее это сделать с помощью `conda`:

```bash
conda create -n vllm_venv python=3.12 -y
conda activate vllm_venv
pip install -r requirements.txt
```
Важно: окружение следует использовать 'пустое' в связи с требованиями установки библиотеки vllm.

### Запуск

#### vllm-сервер + прокси
Ниже будет показан запуск проекта с моделью [DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16](https://huggingface.co/RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16), она же используется по умолчанию. Изначально модель требуется загрузить, для этого будет использоваться huggingface access token, можете использовать тот, что я скину для первичной загрузки весов модели.

⚠️Потом нужно будет создать свой⚠️, либо использовать локальную загрузку весов с диска без синхронизации с huggingface.
Убедитесь, что виртуальное окружение активировано в текущей bash-сессии и запустите скрипт `start_all.sh`:

```bash
bash start_all.sh hf_token=your_token_here model_name=RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16 gpu_count=2 model_load_timeout=6000
```

Данный скрипт запускает vllm-сервер с моделью, после того, как он запустился - производится запуск прокси сервера. Первичный запуск займёт довольно продолжительное время, поскольку сначала vllm должен скачать веса. При последующих запусках время загрузки должно занимать не более 1 - 3 минут.

#### vllm сервер

Если прокси вам не нужен, можете запустить только vllm-сервер и использовать его:
```bash
bash start_vllm.sh hf_token=your_token_here model_name=RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16 gpu_count=2 model_load_timeout=6000
```

#### прокси сервер

Если vllm сервер запущен, можно потом запустить прокси отдельно:

```bash
bash start_proxy.sh
```

#### оффлайн режим

Когда веса успешно скачаны - можно запустить в оффлайн режиме без использования сети:
```bash
bash start_all.sh model_name=RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16 hf_hub_offline=1 gpu_count=2 
```

### Параметры запуска
Для скриптов `start_vllm.sh` и `start_all.sh` определены одинаковые параметры запуска:
* __hf_token__ - токен huggingface, необходим для загрузки модели и её использования не в оффлайн режиме.
* __hf_hub_offline__ - по умолчанию `0`, если указать `1` - будет производится загрузка модели локально с диска без использования сети.
* __model_name__ - название модели (имя на huggingface). По умолчанию `RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16`
* __openai_api_key__ - OpenAI API токен, который использует vllm-сервер. По умолчанию `token-abc123`
* __model_load_timeout__ - таймаут загрузки модели в секундах. По умолчанию `600`
* __gpu_count__ - число используемых видеокарт. По умолчанию `1`
* __hf_home__ - дирректория куда huggingface складывает веса. По умолчанию `~/.cache/huggingface`
* __vllm_cache_root__ - дирректория кэширования для vllm. По умолчанию `~/.cache/vllm`

Для `start_all.sh` ещё можно указать значения портов:

* __proxy_port__ - порт прокси сервера. По умолчанию `8000` 
* __vllm_port__ - порт vllm-сервера. По умолчанию `8001`

Для `start_vllm.sh` можно указать только порт vllm-сервера:

* __vllm_port__ - порт vllm-сервера. По умолчанию `8001`

Для `start_proxy.sh` можно указать только один аргумент - порт:

* __proxy_port__ - порт прокси сервера. По умолчанию `8000`

Параметры запуска указываются, как аргументы командной строки следующим образом: `имя_аргумента=значение_без_пробелов`

⚠️Важно: если вы поменяли `vllm_port` или `openai_api_key` не забудьте поменять их в **settings.json** (это поля `vllm_server_url` и `openai_api_key` соответственно)

### Проверка работы

Для проверки работоспособности запустите скрипт с тестовыми запросами. В них содержатся как валидные запросы, так и невалидные, которые сервер тоже должен обработать:

```bash
python test_requests.py
```

### Завершение работы

в `proxy.pid` и `vllm.pid` будут помещены id процессов для fast-api прокси и для vllm-сервера соответственно. Поскольку оба процесса запускаются в фоновом режиме, требуется сделать для каждого `kill pid` если хотите завершить работу.


## Примеры запросов

### Chat Completion

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer token-abc123" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "messages": [
      {"role": "system", "content": "Ты математический помощник."},
      {"role": "user", "content": "Реши уравнение x^2 = и обоснуй решение."}
    ],
    "max_completion_tokens": 1024,
    "r1_settings": {"few_shot_mode": "PREPROCESS"}
  }'
```

### Completion

```bash
curl http://localhost:8000/v1/completions \
  -H "Authorization: Bearer token-abc123" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16",
    "prompt": "Реши уравнение x^2 = "
  }'
```

### Решение математических задач

Внутренняя инструкция для математического режима представлена на английском языке, поэтому лучше писать на английском.

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer token-abc123" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "RedHatAI/DeepSeek-R1-Distill-Qwen-32B-quantized.w4a16",
    "messages": [
      {"role": "user", "content": "solve the equation: x^3 + 13 - x^2 = 1"}
    ],
    "max_completion_tokens": 2024,
    "r1_settings": {"math_mode": true}
  }'
```
